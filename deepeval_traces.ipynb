{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228ac492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from langfuse import Langfuse\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "langfuse = Langfuse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "839179a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = langfuse.api.trace.get(\"3c7300a2c603e23e4cd87f07cdc5c34f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4532d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '3c7300a2c603e23e4cd87f07cdc5c34f',\n",
       " 'timestamp': datetime.datetime(2025, 11, 20, 9, 28, 20, 473000, tzinfo=datetime.timezone.utc),\n",
       " 'name': 'call_llm',\n",
       " 'input': {'model': 'azure/gpt-4o-mini',\n",
       "  'config': {'system_instruction': '\\n        **Role:** Host agent for the agent-to-agent protocol; delegates queries to specialized remote agents with maximum efficiency.\\n\\n**Core Directives:**\\n\\n* **Task Delegation:** Use the `send_message` function to assign precise, actionable tasks to remote agents.\\n* **Full Context Provision:** If an agent repeatedly asks for user confirmation, it likely lacks conversation history. Include all relevant context in the task to prevent this.\\n* **Autonomous Multi-Agent Engagement:** Engage any required agents directly—never seek user permission or preference. If multiple agents are needed, orchestrate them seamlessly.\\n* **Intelligent Inter-Agent Collaboration:** Instruct agents to determine if they need data from another agent. **If Agent A says \"I need X to proceed\" and Agent B can provide X, immediately query Agent B, then resubmit the updated task to Agent A.**\\n* **Transparent Output:** Deliver the full, unedited response from the final agent(s) to the user.\\n* **Confirmation Handling:** Only relay confirmation requests to the user if not already provided. Never confirm on behalf of the user.\\n* **Minimal Context Sharing:** Share only task-relevant context with each agent. Omit irrelevant details.\\n* **No Redundancy:** Never ask agents to confirm actions or information.\\n* **Tool-Only Responses:** Rely exclusively on tools and agents. If data is missing, request clarification from the user—never assume.\\n* **Recency Priority:** Base decisions on the most recent user message.\\n* **Active Agent Routing:** Route follow-up queries to the currently active agent using task updates.\\n\\n**Efficiency Rule:** \\n> **Dependency Resolution Loop:**  \\n> If Agent A blocks on missing info → Identify Agent B that can supply it → Query B → Feed result back to A → Repeat until A completes or escalates.\\n        **Agent Roster:**\\n\\n        * Available Agents: `{\"name\": \"Capital Agent\", \"description\": \"Answers capital-city questions for any country\"}\\n{\"name\": \"General Q&A Agent\", \"description\": \"Answers everyday questions on any topic \\\\u2014 weather, facts, how-to, recommendations, and more.\"}`\\n        * Currently Active Seller Agent: `Capital Agent`\\n                \\n\\nYou are an agent. Your internal name is \"Routing_agent\". The description about you is \"This Routing agent orchestrates the decomposition of the user asking for department or manager information.\".',\n",
       "   'tools': [{'function_declarations': [{'description': 'Sends a task to remote seller agent.\\n\\nThis will send a message to the remote agent named agent_name.\\n\\nArgs:\\n    agent_name: The name of the agent to send the task to.\\n    task: The comprehensive conversation context summary\\n        and goal to be achieved regarding user inquiry and purchase request.\\n    tool_context: The tool context this method runs in.\\n\\nYields:\\n    A dictionary of JSON data.\\n',\n",
       "       'name': 'send_message',\n",
       "       'parameters': {'properties': {'agent_name': {'type': 'STRING'},\n",
       "         'task': {'type': 'STRING'}},\n",
       "        'required': ['agent_name', 'task'],\n",
       "        'type': 'OBJECT'},\n",
       "       'response': {}}]}],\n",
       "   'labels': {'adk_agent_name': 'Routing_agent'}},\n",
       "  'contents': [{'parts': [{'text': 'what is the capital of Pakistan and write 10 lines about it.'}],\n",
       "    'role': 'user'},\n",
       "   {'parts': [{'function_call': {'id': 'call_2kkSR1RTSkhmWq7mei1gcBgB',\n",
       "       'args': {'agent_name': 'Capital Agent',\n",
       "        'task': 'User requested the capital of Pakistan and a 10-line description about it.'},\n",
       "       'name': 'send_message'}}],\n",
       "    'role': 'model'},\n",
       "   {'parts': [{'function_response': {'id': 'call_2kkSR1RTSkhmWq7mei1gcBgB',\n",
       "       'name': 'send_message',\n",
       "       'response': {'result': {'artifacts': [{'artifactId': '3d9b7caa-07e0-45c0-a38a-cc8e3948cdac',\n",
       "           'parts': [{'kind': 'text', 'text': 'Islamabad'}]}],\n",
       "         'contextId': 'b6b6d2ce-ca8a-43e5-adbe-999eba9ebd1c',\n",
       "         'history': [{'contextId': 'b6b6d2ce-ca8a-43e5-adbe-999eba9ebd1c',\n",
       "           'kind': 'message',\n",
       "           'messageId': '2aff8a91-f6c0-4fe4-8b98-45aff71b2cf3',\n",
       "           'parts': [{'kind': 'text', 'text': 'Islamabad'}],\n",
       "           'role': 'agent'}],\n",
       "         'id': '998fde21-2f6a-4116-8669-b48ad0d278d9',\n",
       "         'kind': 'task',\n",
       "         'status': {'state': 'completed'}}}}}],\n",
       "    'role': 'user'}]},\n",
       " 'output': {'content': {'parts': [{'function_call': {'id': 'call_pQA9a3Ea1lmhaEp6r4EdsWto',\n",
       "      'args': {'agent_name': 'General Q&A Agent',\n",
       "       'task': 'User asked for a 10-line description of Islamabad, the capital of Pakistan.'},\n",
       "      'name': 'send_message'}}],\n",
       "   'role': 'model'},\n",
       "  'partial': False,\n",
       "  'usage_metadata': {'candidates_token_count': 38,\n",
       "   'prompt_token_count': 921,\n",
       "   'total_token_count': 959}},\n",
       " 'metadata': {'attributes': {'gen_ai.system': 'gcp.vertex.agent',\n",
       "   'gen_ai.request.model': 'azure/gpt-4o-mini',\n",
       "   'gcp.vertex.agent.invocation_id': 'e-1773c2c0-9fb0-4741-bf2f-4936346cf4da',\n",
       "   'gcp.vertex.agent.session_id': 'default_session',\n",
       "   'gcp.vertex.agent.event_id': '5de324d5-9cd4-470e-b86c-83dbc92187b3',\n",
       "   'gen_ai.usage.input_tokens': '921',\n",
       "   'gen_ai.usage.output_tokens': '38'},\n",
       "  'resourceAttributes': {'telemetry.sdk.language': 'python',\n",
       "   'telemetry.sdk.name': 'opentelemetry',\n",
       "   'telemetry.sdk.version': '1.38.0',\n",
       "   'service.name': 'unknown_service'},\n",
       "  'scope': {'name': 'gcp.vertex.agent',\n",
       "   'version': '1.17.0',\n",
       "   'attributes': {}}},\n",
       " 'tags': [],\n",
       " 'public': False,\n",
       " 'environment': 'default',\n",
       " 'htmlPath': '/project/cmhwzwyvs01fjad07xfmlp94h/traces/3c7300a2c603e23e4cd87f07cdc5c34f',\n",
       " 'latency': 8.731,\n",
       " 'totalCost': 0.0,\n",
       " 'observations': [{'id': '6eda86706e8e2b96',\n",
       "   'traceId': '3c7300a2c603e23e4cd87f07cdc5c34f',\n",
       "   'type': 'AGENT',\n",
       "   'name': 'rohit',\n",
       "   'startTime': datetime.datetime(2025, 11, 20, 9, 28, 23, 210000, tzinfo=datetime.timezone.utc),\n",
       "   'endTime': datetime.datetime(2025, 11, 20, 9, 28, 29, 196000, tzinfo=datetime.timezone.utc),\n",
       "   'completionStartTime': None,\n",
       "   'model': None,\n",
       "   'modelParameters': {},\n",
       "   'input': {'args': [],\n",
       "    'kwargs': {'message_request': {'id': 'ed341398-ed9e-4d38-bf1f-2676650741c3',\n",
       "      'jsonrpc': '2.0',\n",
       "      'method': 'message/send',\n",
       "      'params': {'configuration': None,\n",
       "       'message': {'contextId': '2b2860c5-50fe-4b8e-9ba6-cc77817fd6d7',\n",
       "        'extensions': None,\n",
       "        'kind': 'message',\n",
       "        'messageId': 'ed341398-ed9e-4d38-bf1f-2676650741c3',\n",
       "        'metadata': None,\n",
       "        'parts': [{'kind': 'text',\n",
       "          'metadata': None,\n",
       "          'text': 'User asked for a 10-line description of Islamabad, the capital of Pakistan.'}],\n",
       "        'referenceTaskIds': None,\n",
       "        'role': 'user',\n",
       "        'taskId': None},\n",
       "       'metadata': None}}}},\n",
       "   'version': None,\n",
       "   'metadata': {'resourceAttributes': {'telemetry.sdk.language': 'python',\n",
       "     'telemetry.sdk.name': 'opentelemetry',\n",
       "     'telemetry.sdk.version': '1.38.0',\n",
       "     'service.name': 'unknown_service'},\n",
       "    'scope': {'name': 'langfuse-sdk',\n",
       "     'version': '3.10.0',\n",
       "     'attributes': {'public_key': 'pk-lf-f69fce2c-2a85-44c6-8777-646b07f7ccaf'}}},\n",
       "   'output': {'id': 'ed341398-ed9e-4d38-bf1f-2676650741c3',\n",
       "    'jsonrpc': '2.0',\n",
       "    'result': {'artifacts': [{'artifactId': '22299dc1-62cd-4858-aa76-32eb0b695121',\n",
       "       'description': None,\n",
       "       'extensions': None,\n",
       "       'metadata': None,\n",
       "       'name': None,\n",
       "       'parts': [{'kind': 'text',\n",
       "         'metadata': None,\n",
       "         'text': \"Islamabad is the capital city of Pakistan, officially established in 1967 to replace Karachi as the nation's capital. Located in the northern part of the country near the Margalla Hills, the city was purposefully designed and built as a modern planned capital. The city is known for its well-organized layout, wide tree-lined avenues, and systematic sectoral divisions that create an orderly urban environment. Islamabad houses important government buildings, including the Presidential Palace, Parliament House, and Supreme Court of Pakistan. The city is characterized by its clean, green environment with numerous parks, gardens, and the scenic Margalla Hills National Park nearby. Home to several prestigious educational institutions like Quaid-i-Azam University and the International Islamic University, it serves as an important academic center. The population is diverse, consisting of government officials, diplomats, professionals, and students from across Pakistan and abroad. Islamabad's economy is primarily based on government services, though it also has growing technology and business sectors. The city experiences a humid subtropical climate with hot summers and mild winters. As Pakistan's political and administrative center, Islamabad plays a crucial role in the country's governance and international diplomatic relations.\"}]}],\n",
       "     'contextId': '2b2860c5-50fe-4b8e-9ba6-cc77817fd6d7',\n",
       "     'history': [{'contextId': '2b2860c5-50fe-4b8e-9ba6-cc77817fd6d7',\n",
       "       'extensions': None,\n",
       "       'kind': 'message',\n",
       "       'messageId': 'ed341398-ed9e-4d38-bf1f-2676650741c3',\n",
       "       'metadata': None,\n",
       "       'parts': [{'kind': 'text',\n",
       "         'metadata': None,\n",
       "         'text': \"Islamabad is the capital city of Pakistan, officially established in 1967 to replace Karachi as the nation's capital. Located in the northern part of the country near the Margalla Hills, the city was purposefully designed and built as a modern planned capital. The city is known for its well-organized layout, wide tree-lined avenues, and systematic sectoral divisions that create an orderly urban environment. Islamabad houses important government buildings, including the Presidential Palace, Parliament House, and Supreme Court of Pakistan. The city is characterized by its clean, green environment with numerous parks, gardens, and the scenic Margalla Hills National Park nearby. Home to several prestigious educational institutions like Quaid-i-Azam University and the International Islamic University, it serves as an important academic center. The population is diverse, consisting of government officials, diplomats, professionals, and students from across Pakistan and abroad. Islamabad's economy is primarily based on government services, though it also has growing technology and business sectors. The city experiences a humid subtropical climate with hot summers and mild winters. As Pakistan's political and administrative center, Islamabad plays a crucial role in the country's governance and international diplomatic relations.\"}],\n",
       "       'referenceTaskIds': None,\n",
       "       'role': 'agent',\n",
       "       'taskId': None}],\n",
       "     'id': 'fac87679-acbf-4405-bfb3-36f7bd9e3f3a',\n",
       "     'kind': 'task',\n",
       "     'metadata': None,\n",
       "     'status': {'message': None, 'state': 'completed', 'timestamp': None}}},\n",
       "   'usage': {'input': 0,\n",
       "    'output': 0,\n",
       "    'total': 0,\n",
       "    'unit': <ModelUsageUnit.TOKENS: 'TOKENS'>},\n",
       "   'level': <ObservationLevel.DEFAULT: 'DEFAULT'>,\n",
       "   'statusMessage': None,\n",
       "   'parentObservationId': 'bfd858f112cad05f',\n",
       "   'promptId': None,\n",
       "   'usageDetails': {},\n",
       "   'costDetails': {},\n",
       "   'environment': 'default',\n",
       "   'promptName': None,\n",
       "   'promptVersion': None,\n",
       "   'modelId': None,\n",
       "   'inputPrice': 0.0,\n",
       "   'outputPrice': 0.0,\n",
       "   'totalPrice': 0.0,\n",
       "   'calculatedInputCost': None,\n",
       "   'calculatedOutputCost': None,\n",
       "   'calculatedTotalCost': 0.0,\n",
       "   'latency': 5986.0,\n",
       "   'timeToFirstToken': None,\n",
       "   'promptTokens': 0,\n",
       "   'unit': 'TOKENS',\n",
       "   'createdAt': '2025-11-20T09:28:31.988Z',\n",
       "   'updatedAt': '2025-11-20T09:28:31.988Z',\n",
       "   'totalTokens': 0,\n",
       "   'projectId': 'cmhwzwyvs01fjad07xfmlp94h',\n",
       "   'completionTokens': 0},\n",
       "  {'id': 'a9e7e75088e2edca',\n",
       "   'traceId': '3c7300a2c603e23e4cd87f07cdc5c34f',\n",
       "   'type': 'GENERATION',\n",
       "   'name': 'call_llm',\n",
       "   'startTime': datetime.datetime(2025, 11, 20, 9, 28, 20, 473000, tzinfo=datetime.timezone.utc),\n",
       "   'endTime': datetime.datetime(2025, 11, 20, 9, 28, 29, 204000, tzinfo=datetime.timezone.utc),\n",
       "   'completionStartTime': None,\n",
       "   'model': 'azure/gpt-4o-mini',\n",
       "   'modelParameters': {},\n",
       "   'input': {'model': 'azure/gpt-4o-mini',\n",
       "    'config': {'system_instruction': '\\n        **Role:** Host agent for the agent-to-agent protocol; delegates queries to specialized remote agents with maximum efficiency.\\n\\n**Core Directives:**\\n\\n* **Task Delegation:** Use the `send_message` function to assign precise, actionable tasks to remote agents.\\n* **Full Context Provision:** If an agent repeatedly asks for user confirmation, it likely lacks conversation history. Include all relevant context in the task to prevent this.\\n* **Autonomous Multi-Agent Engagement:** Engage any required agents directly—never seek user permission or preference. If multiple agents are needed, orchestrate them seamlessly.\\n* **Intelligent Inter-Agent Collaboration:** Instruct agents to determine if they need data from another agent. **If Agent A says \"I need X to proceed\" and Agent B can provide X, immediately query Agent B, then resubmit the updated task to Agent A.**\\n* **Transparent Output:** Deliver the full, unedited response from the final agent(s) to the user.\\n* **Confirmation Handling:** Only relay confirmation requests to the user if not already provided. Never confirm on behalf of the user.\\n* **Minimal Context Sharing:** Share only task-relevant context with each agent. Omit irrelevant details.\\n* **No Redundancy:** Never ask agents to confirm actions or information.\\n* **Tool-Only Responses:** Rely exclusively on tools and agents. If data is missing, request clarification from the user—never assume.\\n* **Recency Priority:** Base decisions on the most recent user message.\\n* **Active Agent Routing:** Route follow-up queries to the currently active agent using task updates.\\n\\n**Efficiency Rule:** \\n> **Dependency Resolution Loop:**  \\n> If Agent A blocks on missing info → Identify Agent B that can supply it → Query B → Feed result back to A → Repeat until A completes or escalates.\\n        **Agent Roster:**\\n\\n        * Available Agents: `{\"name\": \"Capital Agent\", \"description\": \"Answers capital-city questions for any country\"}\\n{\"name\": \"General Q&A Agent\", \"description\": \"Answers everyday questions on any topic \\\\u2014 weather, facts, how-to, recommendations, and more.\"}`\\n        * Currently Active Seller Agent: `Capital Agent`\\n                \\n\\nYou are an agent. Your internal name is \"Routing_agent\". The description about you is \"This Routing agent orchestrates the decomposition of the user asking for department or manager information.\".',\n",
       "     'tools': [{'function_declarations': [{'description': 'Sends a task to remote seller agent.\\n\\nThis will send a message to the remote agent named agent_name.\\n\\nArgs:\\n    agent_name: The name of the agent to send the task to.\\n    task: The comprehensive conversation context summary\\n        and goal to be achieved regarding user inquiry and purchase request.\\n    tool_context: The tool context this method runs in.\\n\\nYields:\\n    A dictionary of JSON data.\\n',\n",
       "         'name': 'send_message',\n",
       "         'parameters': {'properties': {'agent_name': {'type': 'STRING'},\n",
       "           'task': {'type': 'STRING'}},\n",
       "          'required': ['agent_name', 'task'],\n",
       "          'type': 'OBJECT'},\n",
       "         'response': {}}]}],\n",
       "     'labels': {'adk_agent_name': 'Routing_agent'}},\n",
       "    'contents': [{'parts': [{'text': 'what is the capital of Pakistan and write 10 lines about it.'}],\n",
       "      'role': 'user'},\n",
       "     {'parts': [{'function_call': {'id': 'call_2kkSR1RTSkhmWq7mei1gcBgB',\n",
       "         'args': {'agent_name': 'Capital Agent',\n",
       "          'task': 'User requested the capital of Pakistan and a 10-line description about it.'},\n",
       "         'name': 'send_message'}}],\n",
       "      'role': 'model'},\n",
       "     {'parts': [{'function_response': {'id': 'call_2kkSR1RTSkhmWq7mei1gcBgB',\n",
       "         'name': 'send_message',\n",
       "         'response': {'result': {'artifacts': [{'artifactId': '3d9b7caa-07e0-45c0-a38a-cc8e3948cdac',\n",
       "             'parts': [{'kind': 'text', 'text': 'Islamabad'}]}],\n",
       "           'contextId': 'b6b6d2ce-ca8a-43e5-adbe-999eba9ebd1c',\n",
       "           'history': [{'contextId': 'b6b6d2ce-ca8a-43e5-adbe-999eba9ebd1c',\n",
       "             'kind': 'message',\n",
       "             'messageId': '2aff8a91-f6c0-4fe4-8b98-45aff71b2cf3',\n",
       "             'parts': [{'kind': 'text', 'text': 'Islamabad'}],\n",
       "             'role': 'agent'}],\n",
       "           'id': '998fde21-2f6a-4116-8669-b48ad0d278d9',\n",
       "           'kind': 'task',\n",
       "           'status': {'state': 'completed'}}}}}],\n",
       "      'role': 'user'}]},\n",
       "   'version': None,\n",
       "   'metadata': {'attributes': {'gen_ai.system': 'gcp.vertex.agent',\n",
       "     'gen_ai.request.model': 'azure/gpt-4o-mini',\n",
       "     'gcp.vertex.agent.invocation_id': 'e-1773c2c0-9fb0-4741-bf2f-4936346cf4da',\n",
       "     'gcp.vertex.agent.session_id': 'default_session',\n",
       "     'gcp.vertex.agent.event_id': '5de324d5-9cd4-470e-b86c-83dbc92187b3',\n",
       "     'gen_ai.usage.input_tokens': '921',\n",
       "     'gen_ai.usage.output_tokens': '38'},\n",
       "    'resourceAttributes': {'telemetry.sdk.language': 'python',\n",
       "     'telemetry.sdk.name': 'opentelemetry',\n",
       "     'telemetry.sdk.version': '1.38.0',\n",
       "     'service.name': 'unknown_service'},\n",
       "    'scope': {'name': 'gcp.vertex.agent',\n",
       "     'version': '1.17.0',\n",
       "     'attributes': {}}},\n",
       "   'output': {'content': {'parts': [{'function_call': {'id': 'call_pQA9a3Ea1lmhaEp6r4EdsWto',\n",
       "        'args': {'agent_name': 'General Q&A Agent',\n",
       "         'task': 'User asked for a 10-line description of Islamabad, the capital of Pakistan.'},\n",
       "        'name': 'send_message'}}],\n",
       "     'role': 'model'},\n",
       "    'partial': False,\n",
       "    'usage_metadata': {'candidates_token_count': 38,\n",
       "     'prompt_token_count': 921,\n",
       "     'total_token_count': 959}},\n",
       "   'usage': {'input': 921,\n",
       "    'output': 38,\n",
       "    'total': 959,\n",
       "    'unit': <ModelUsageUnit.TOKENS: 'TOKENS'>},\n",
       "   'level': <ObservationLevel.DEFAULT: 'DEFAULT'>,\n",
       "   'statusMessage': None,\n",
       "   'parentObservationId': None,\n",
       "   'promptId': None,\n",
       "   'usageDetails': {'input': 921, 'output': 38, 'total': 959},\n",
       "   'costDetails': {},\n",
       "   'environment': 'default',\n",
       "   'promptName': None,\n",
       "   'promptVersion': None,\n",
       "   'modelId': None,\n",
       "   'inputPrice': 0.0,\n",
       "   'outputPrice': 0.0,\n",
       "   'totalPrice': 0.0,\n",
       "   'calculatedInputCost': None,\n",
       "   'calculatedOutputCost': None,\n",
       "   'calculatedTotalCost': 0.0,\n",
       "   'latency': 8731.0,\n",
       "   'timeToFirstToken': None,\n",
       "   'promptTokens': 921,\n",
       "   'unit': 'TOKENS',\n",
       "   'createdAt': '2025-11-20T09:28:31.988Z',\n",
       "   'updatedAt': '2025-11-20T09:28:31.988Z',\n",
       "   'totalTokens': 959,\n",
       "   'projectId': 'cmhwzwyvs01fjad07xfmlp94h',\n",
       "   'completionTokens': 38},\n",
       "  {'id': '36d37b6e55229026',\n",
       "   'traceId': '3c7300a2c603e23e4cd87f07cdc5c34f',\n",
       "   'type': 'SPAN',\n",
       "   'name': 'a2a.client.transports.jsonrpc.JsonRpcTransport.send_message',\n",
       "   'startTime': datetime.datetime(2025, 11, 20, 9, 28, 23, 210000, tzinfo=datetime.timezone.utc),\n",
       "   'endTime': datetime.datetime(2025, 11, 20, 9, 28, 29, 196000, tzinfo=datetime.timezone.utc),\n",
       "   'completionStartTime': None,\n",
       "   'model': None,\n",
       "   'modelParameters': None,\n",
       "   'input': None,\n",
       "   'version': None,\n",
       "   'metadata': {'attributes': {},\n",
       "    'resourceAttributes': {'telemetry.sdk.language': 'python',\n",
       "     'telemetry.sdk.name': 'opentelemetry',\n",
       "     'telemetry.sdk.version': '1.38.0',\n",
       "     'service.name': 'unknown_service'},\n",
       "    'scope': {'name': 'a2a-python-sdk', 'version': '1.0.0', 'attributes': {}}},\n",
       "   'output': None,\n",
       "   'usage': {'input': 0,\n",
       "    'output': 0,\n",
       "    'total': 0,\n",
       "    'unit': <ModelUsageUnit.TOKENS: 'TOKENS'>},\n",
       "   'level': <ObservationLevel.DEFAULT: 'DEFAULT'>,\n",
       "   'statusMessage': None,\n",
       "   'parentObservationId': '6eda86706e8e2b96',\n",
       "   'promptId': None,\n",
       "   'usageDetails': {},\n",
       "   'costDetails': {},\n",
       "   'environment': 'default',\n",
       "   'promptName': None,\n",
       "   'promptVersion': None,\n",
       "   'modelId': None,\n",
       "   'inputPrice': 0.0,\n",
       "   'outputPrice': 0.0,\n",
       "   'totalPrice': 0.0,\n",
       "   'calculatedInputCost': None,\n",
       "   'calculatedOutputCost': None,\n",
       "   'calculatedTotalCost': 0.0,\n",
       "   'latency': 5986.0,\n",
       "   'timeToFirstToken': None,\n",
       "   'promptTokens': 0,\n",
       "   'unit': 'TOKENS',\n",
       "   'createdAt': '2025-11-20T09:28:31.988Z',\n",
       "   'updatedAt': '2025-11-20T09:28:31.988Z',\n",
       "   'totalTokens': 0,\n",
       "   'projectId': 'cmhwzwyvs01fjad07xfmlp94h',\n",
       "   'completionTokens': 0},\n",
       "  {'id': '8b881a2370954903',\n",
       "   'traceId': '3c7300a2c603e23e4cd87f07cdc5c34f',\n",
       "   'type': 'SPAN',\n",
       "   'name': 'a2a.client.transports.jsonrpc.JsonRpcTransport._send_request',\n",
       "   'startTime': datetime.datetime(2025, 11, 20, 9, 28, 23, 211000, tzinfo=datetime.timezone.utc),\n",
       "   'endTime': datetime.datetime(2025, 11, 20, 9, 28, 29, 196000, tzinfo=datetime.timezone.utc),\n",
       "   'completionStartTime': None,\n",
       "   'model': None,\n",
       "   'modelParameters': None,\n",
       "   'input': None,\n",
       "   'version': None,\n",
       "   'metadata': {'attributes': {},\n",
       "    'resourceAttributes': {'telemetry.sdk.language': 'python',\n",
       "     'telemetry.sdk.name': 'opentelemetry',\n",
       "     'telemetry.sdk.version': '1.38.0',\n",
       "     'service.name': 'unknown_service'},\n",
       "    'scope': {'name': 'a2a-python-sdk', 'version': '1.0.0', 'attributes': {}}},\n",
       "   'output': None,\n",
       "   'usage': {'input': 0,\n",
       "    'output': 0,\n",
       "    'total': 0,\n",
       "    'unit': <ModelUsageUnit.TOKENS: 'TOKENS'>},\n",
       "   'level': <ObservationLevel.DEFAULT: 'DEFAULT'>,\n",
       "   'statusMessage': None,\n",
       "   'parentObservationId': '36d37b6e55229026',\n",
       "   'promptId': None,\n",
       "   'usageDetails': {},\n",
       "   'costDetails': {},\n",
       "   'environment': 'default',\n",
       "   'promptName': None,\n",
       "   'promptVersion': None,\n",
       "   'modelId': None,\n",
       "   'inputPrice': 0.0,\n",
       "   'outputPrice': 0.0,\n",
       "   'totalPrice': 0.0,\n",
       "   'calculatedInputCost': None,\n",
       "   'calculatedOutputCost': None,\n",
       "   'calculatedTotalCost': 0.0,\n",
       "   'latency': 5985.0,\n",
       "   'timeToFirstToken': None,\n",
       "   'promptTokens': 0,\n",
       "   'unit': 'TOKENS',\n",
       "   'createdAt': '2025-11-20T09:28:31.988Z',\n",
       "   'updatedAt': '2025-11-20T09:28:31.988Z',\n",
       "   'totalTokens': 0,\n",
       "   'projectId': 'cmhwzwyvs01fjad07xfmlp94h',\n",
       "   'completionTokens': 0},\n",
       "  {'id': 'bfd858f112cad05f',\n",
       "   'traceId': '3c7300a2c603e23e4cd87f07cdc5c34f',\n",
       "   'type': 'TOOL',\n",
       "   'name': 'execute_tool send_message',\n",
       "   'startTime': datetime.datetime(2025, 11, 20, 9, 28, 23, 209000, tzinfo=datetime.timezone.utc),\n",
       "   'endTime': datetime.datetime(2025, 11, 20, 9, 28, 29, 199000, tzinfo=datetime.timezone.utc),\n",
       "   'completionStartTime': None,\n",
       "   'model': None,\n",
       "   'modelParameters': {},\n",
       "   'input': {'agent_name': 'General Q&A Agent',\n",
       "    'task': 'User asked for a 10-line description of Islamabad, the capital of Pakistan.'},\n",
       "   'version': None,\n",
       "   'metadata': {'attributes': {'gen_ai.operation.name': 'execute_tool',\n",
       "     'gen_ai.tool.description': 'Sends a task to remote seller agent.\\n\\nThis will send a message to the remote agent named agent_name.\\n\\nArgs:\\n    agent_name: The name of the agent to send the task to.\\n    task: The comprehensive conversation context summary\\n        and goal to be achieved regarding user inquiry and purchase request.\\n    tool_context: The tool context this method runs in.\\n\\nYields:\\n    A dictionary of JSON data.',\n",
       "     'gen_ai.tool.name': 'send_message',\n",
       "     'gen_ai.tool.type': 'FunctionTool',\n",
       "     'gen_ai.tool.call.id': 'call_pQA9a3Ea1lmhaEp6r4EdsWto',\n",
       "     'gcp.vertex.agent.event_id': '0e7c0a2c-01dc-48f4-9730-70b176b8cb9e'},\n",
       "    'resourceAttributes': {'telemetry.sdk.language': 'python',\n",
       "     'telemetry.sdk.name': 'opentelemetry',\n",
       "     'telemetry.sdk.version': '1.38.0',\n",
       "     'service.name': 'unknown_service'},\n",
       "    'scope': {'name': 'gcp.vertex.agent',\n",
       "     'version': '1.17.0',\n",
       "     'attributes': {}}},\n",
       "   'output': {'result': '<not serializable>'},\n",
       "   'usage': {'input': 0,\n",
       "    'output': 0,\n",
       "    'total': 0,\n",
       "    'unit': <ModelUsageUnit.TOKENS: 'TOKENS'>},\n",
       "   'level': <ObservationLevel.DEFAULT: 'DEFAULT'>,\n",
       "   'statusMessage': None,\n",
       "   'parentObservationId': 'a9e7e75088e2edca',\n",
       "   'promptId': None,\n",
       "   'usageDetails': {},\n",
       "   'costDetails': {},\n",
       "   'environment': 'default',\n",
       "   'promptName': None,\n",
       "   'promptVersion': None,\n",
       "   'modelId': None,\n",
       "   'inputPrice': 0.0,\n",
       "   'outputPrice': 0.0,\n",
       "   'totalPrice': 0.0,\n",
       "   'calculatedInputCost': None,\n",
       "   'calculatedOutputCost': None,\n",
       "   'calculatedTotalCost': 0.0,\n",
       "   'latency': 5990.0,\n",
       "   'timeToFirstToken': None,\n",
       "   'promptTokens': 0,\n",
       "   'unit': 'TOKENS',\n",
       "   'createdAt': '2025-11-20T09:28:31.988Z',\n",
       "   'updatedAt': '2025-11-20T09:28:31.988Z',\n",
       "   'totalTokens': 0,\n",
       "   'projectId': 'cmhwzwyvs01fjad07xfmlp94h',\n",
       "   'completionTokens': 0},\n",
       "  {'id': 'd28642acb4e842bb',\n",
       "   'traceId': '3c7300a2c603e23e4cd87f07cdc5c34f',\n",
       "   'type': 'SPAN',\n",
       "   'name': 'a2a.client.transports.jsonrpc.JsonRpcTransport._apply_interceptors',\n",
       "   'startTime': datetime.datetime(2025, 11, 20, 9, 28, 23, 211000, tzinfo=datetime.timezone.utc),\n",
       "   'endTime': datetime.datetime(2025, 11, 20, 9, 28, 23, 211000, tzinfo=datetime.timezone.utc),\n",
       "   'completionStartTime': None,\n",
       "   'model': None,\n",
       "   'modelParameters': None,\n",
       "   'input': None,\n",
       "   'version': None,\n",
       "   'metadata': {'attributes': {},\n",
       "    'resourceAttributes': {'telemetry.sdk.language': 'python',\n",
       "     'telemetry.sdk.name': 'opentelemetry',\n",
       "     'telemetry.sdk.version': '1.38.0',\n",
       "     'service.name': 'unknown_service'},\n",
       "    'scope': {'name': 'a2a-python-sdk', 'version': '1.0.0', 'attributes': {}}},\n",
       "   'output': None,\n",
       "   'usage': {'input': 0,\n",
       "    'output': 0,\n",
       "    'total': 0,\n",
       "    'unit': <ModelUsageUnit.TOKENS: 'TOKENS'>},\n",
       "   'level': <ObservationLevel.DEFAULT: 'DEFAULT'>,\n",
       "   'statusMessage': None,\n",
       "   'parentObservationId': '36d37b6e55229026',\n",
       "   'promptId': None,\n",
       "   'usageDetails': {},\n",
       "   'costDetails': {},\n",
       "   'environment': 'default',\n",
       "   'promptName': None,\n",
       "   'promptVersion': None,\n",
       "   'modelId': None,\n",
       "   'inputPrice': 0.0,\n",
       "   'outputPrice': 0.0,\n",
       "   'totalPrice': 0.0,\n",
       "   'calculatedInputCost': None,\n",
       "   'calculatedOutputCost': None,\n",
       "   'calculatedTotalCost': 0.0,\n",
       "   'latency': 0.0,\n",
       "   'timeToFirstToken': None,\n",
       "   'promptTokens': 0,\n",
       "   'unit': 'TOKENS',\n",
       "   'createdAt': '2025-11-20T09:28:26.608Z',\n",
       "   'updatedAt': '2025-11-20T09:28:26.609Z',\n",
       "   'totalTokens': 0,\n",
       "   'projectId': 'cmhwzwyvs01fjad07xfmlp94h',\n",
       "   'completionTokens': 0},\n",
       "  {'id': '2505fd8e73f7c16d',\n",
       "   'traceId': '3c7300a2c603e23e4cd87f07cdc5c34f',\n",
       "   'type': 'SPAN',\n",
       "   'name': 'a2a.client.transports.jsonrpc.JsonRpcTransport._get_http_args',\n",
       "   'startTime': datetime.datetime(2025, 11, 20, 9, 28, 23, 210000, tzinfo=datetime.timezone.utc),\n",
       "   'endTime': datetime.datetime(2025, 11, 20, 9, 28, 23, 210000, tzinfo=datetime.timezone.utc),\n",
       "   'completionStartTime': None,\n",
       "   'model': None,\n",
       "   'modelParameters': None,\n",
       "   'input': None,\n",
       "   'version': None,\n",
       "   'metadata': {'attributes': {},\n",
       "    'resourceAttributes': {'telemetry.sdk.language': 'python',\n",
       "     'telemetry.sdk.name': 'opentelemetry',\n",
       "     'telemetry.sdk.version': '1.38.0',\n",
       "     'service.name': 'unknown_service'},\n",
       "    'scope': {'name': 'a2a-python-sdk', 'attributes': {}}},\n",
       "   'output': None,\n",
       "   'usage': {'input': 0,\n",
       "    'output': 0,\n",
       "    'total': 0,\n",
       "    'unit': <ModelUsageUnit.TOKENS: 'TOKENS'>},\n",
       "   'level': <ObservationLevel.DEFAULT: 'DEFAULT'>,\n",
       "   'statusMessage': None,\n",
       "   'parentObservationId': '36d37b6e55229026',\n",
       "   'promptId': None,\n",
       "   'usageDetails': {},\n",
       "   'costDetails': {},\n",
       "   'environment': 'default',\n",
       "   'promptName': None,\n",
       "   'promptVersion': None,\n",
       "   'modelId': None,\n",
       "   'inputPrice': 0.0,\n",
       "   'outputPrice': 0.0,\n",
       "   'totalPrice': 0.0,\n",
       "   'calculatedInputCost': None,\n",
       "   'calculatedOutputCost': None,\n",
       "   'calculatedTotalCost': 0.0,\n",
       "   'latency': 0.0,\n",
       "   'timeToFirstToken': None,\n",
       "   'promptTokens': 0,\n",
       "   'unit': 'TOKENS',\n",
       "   'createdAt': '2025-11-20T09:28:26.608Z',\n",
       "   'updatedAt': '2025-11-20T09:28:26.608Z',\n",
       "   'totalTokens': 0,\n",
       "   'projectId': 'cmhwzwyvs01fjad07xfmlp94h',\n",
       "   'completionTokens': 0}],\n",
       " 'scores': [],\n",
       " 'bookmarked': False,\n",
       " 'createdAt': '2025-11-20T09:28:27.000Z',\n",
       " 'updatedAt': '2025-11-20T09:28:32.069Z',\n",
       " 'projectId': 'cmhwzwyvs01fjad07xfmlp94h',\n",
       " 'sessionId': None,\n",
       " 'release': None,\n",
       " 'version': None,\n",
       " 'userId': None,\n",
       " 'externalId': None}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f96e8bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dump() missing 1 required positional argument: 'fp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# y = x.dict()\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: dump() missing 1 required positional argument: 'fp'"
     ]
    }
   ],
   "source": [
    "# y = x.dict()\n",
    "import json\n",
    "y = json.dump(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c35aadc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['following is the system instructions of agent named : routing agent', ' ', '        **Role:** Host agent for the agent-to-agent protocol; delegates queries to specialized remote agents with maximum efficiency.', '', '**Core Directives:**', '', '* **Task Delegation:** Use the `send_message` function to assign precise, actionable tasks to remote agents.', '* **Full Context Provision:** If an agent repeatedly asks for user confirmation, it likely lacks conversation history. Include all relevant context in the task to prevent this.', '* **Autonomous Multi-Agent Engagement:** Engage any required agents directly—never seek user permission or preference. If multiple agents are needed, orchestrate them seamlessly.', '* **Intelligent Inter-Agent Collaboration:** Instruct agents to determine if they need data from another agent. **If Agent A says \"I need X to proceed\" and Agent B can provide X, immediately query Agent B, then resubmit the updated task to Agent A.**', '* **Transparent Output:** Deliver the full, unedited response from the final agent(s) to the user.', '* **Confirmation Handling:** Only relay confirmation requests to the user if not already provided. Never confirm on behalf of the user.', '* **Minimal Context Sharing:** Share only task-relevant context with each agent. Omit irrelevant details.', '* **No Redundancy:** Never ask agents to confirm actions or information.', '* **Tool-Only Responses:** Rely exclusively on tools and agents. If data is missing, request clarification from the user—never assume.', '* **Recency Priority:** Base decisions on the most recent user message.', '* **Active Agent Routing:** Route follow-up queries to the currently active agent using task updates.', '', '**Efficiency Rule:** ', '> **Dependency Resolution Loop:**  ', '> If Agent A blocks on missing info → Identify Agent B that can supply it → Query B → Feed result back to A → Repeat until A completes or escalates.', '        **Agent Roster:**', '', '        * Available Agents: `{\"name\": \"Capital Agent\", \"description\": \"Answers capital-city questions for any country\"}', '{\"name\": \"General Q&A Agent\", \"description\": \"Answers everyday questions on any topic \\\\u2014 weather, facts, how-to, recommendations, and more.\"}`', '        * Currently Active Seller Agent: `Capital Agent`', '                ', '', 'You are an agent. Your internal name is \"Routing_agent\". The description about you is \"This Routing agent orchestrates the decomposition of the user asking for department or manager information.\".']\n",
      "following is the system instructions of agent named : routing agent\n",
      " \n",
      "        **Role:** Host agent for the agent-to-agent protocol; delegates queries to specialized remote agents with maximum efficiency.\n",
      "\n",
      "**Core Directives:**\n",
      "\n",
      "* **Task Delegation:** Use the `send_message` function to assign precise, actionable tasks to remote agents.\n",
      "* **Full Context Provision:** If an agent repeatedly asks for user confirmation, it likely lacks conversation history. Include all relevant context in the task to prevent this.\n",
      "* **Autonomous Multi-Agent Engagement:** Engage any required agents directly—never seek user permission or preference. If multiple agents are needed, orchestrate them seamlessly.\n",
      "* **Intelligent Inter-Agent Collaboration:** Instruct agents to determine if they need data from another agent. **If Agent A says \"I need X to proceed\" and Agent B can provide X, immediately query Agent B, then resubmit the updated task to Agent A.**\n",
      "* **Transparent Output:** Deliver the full, unedited response from the final agent(s) to the user.\n",
      "* **Confirmation Handling:** Only relay confirmation requests to the user if not already provided. Never confirm on behalf of the user.\n",
      "* **Minimal Context Sharing:** Share only task-relevant context with each agent. Omit irrelevant details.\n",
      "* **No Redundancy:** Never ask agents to confirm actions or information.\n",
      "* **Tool-Only Responses:** Rely exclusively on tools and agents. If data is missing, request clarification from the user—never assume.\n",
      "* **Recency Priority:** Base decisions on the most recent user message.\n",
      "* **Active Agent Routing:** Route follow-up queries to the currently active agent using task updates.\n",
      "\n",
      "**Efficiency Rule:** \n",
      "> **Dependency Resolution Loop:**  \n",
      "> If Agent A blocks on missing info → Identify Agent B that can supply it → Query B → Feed result back to A → Repeat until A completes or escalates.\n",
      "        **Agent Roster:**\n",
      "\n",
      "        * Available Agents: `{\"name\": \"Capital Agent\", \"description\": \"Answers capital-city questions for any country\"}\n",
      "{\"name\": \"General Q&A Agent\", \"description\": \"Answers everyday questions on any topic \\u2014 weather, facts, how-to, recommendations, and more.\"}`\n",
      "        * Currently Active Seller Agent: `Capital Agent`\n",
      "                \n",
      "\n",
      "You are an agent. Your internal name is \"Routing_agent\". The description about you is \"This Routing agent orchestrates the decomposition of the user asking for department or manager information.\".\n"
     ]
    }
   ],
   "source": [
    "context = \"following is the system instructions of agent named : routing agent\\n \"+y[\"input\"][\"config\"][\"system_instruction\"]\n",
    "lines = context.split(\"\\n\")\n",
    "print(lines)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52128d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the capital of Pakistan and write 10 lines about it.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input= y[\"input\"][\"contents\"][0][\"parts\"][0][\"text\"]\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bb472dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Islamabad is the capital city of Pakistan, officially established in 1967 to replace Karachi as the nation's capital. Located in the northern part of the country near the Margalla Hills, the city was purposefully designed and built as a modern planned capital. The city is known for its well-organized layout, wide tree-lined avenues, and systematic sectoral divisions that create an orderly urban environment. Islamabad houses important government buildings, including the Presidential Palace, Parliament House, and Supreme Court of Pakistan. The city is characterized by its clean, green environment with numerous parks, gardens, and the scenic Margalla Hills National Park nearby. Home to several prestigious educational institutions like Quaid-i-Azam University and the International Islamic University, it serves as an important academic center. The population is diverse, consisting of government officials, diplomats, professionals, and students from across Pakistan and abroad. Islamabad's economy is primarily based on government services, though it also has growing technology and business sectors. The city experiences a humid subtropical climate with hot summers and mild winters. As Pakistan's political and administrative center, Islamabad plays a crucial role in the country's governance and international diplomatic relations.\n"
     ]
    }
   ],
   "source": [
    "output=y[\"observations\"][0][\"output\"][\"result\"][\"artifacts\"][0][\"parts\"][0][\"text\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d757a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Agent 'Capital Agent' is assigned a task User requested the capital of \"\n",
      " 'Pakistan and a 10-line description about it.',\n",
      " 'Agent response: Islamabad',\n",
      " \"Final Agent 'General Q&A Agent' was assigned a task User asked for a 10-line \"\n",
      " 'description of Islamabad, the capital of Pakistan.']\n"
     ]
    }
   ],
   "source": [
    "steps = []\n",
    "import pprint\n",
    "for item in y[\"input\"][\"contents\"]:\n",
    "    parts = item.get(\"parts\", [])\n",
    "    for part in parts:\n",
    "        if \"function_call\" in part:\n",
    "            args = part[\"function_call\"][\"args\"]\n",
    "            steps.append(f\"Agent '{args.get('agent_name')}' is assigned a task {args.get('task')}\")\n",
    "\n",
    "        if \"function_response\" in part:\n",
    "            artifacts = part[\"function_response\"][\"response\"][\"result\"][\"artifacts\"]\n",
    "            collected_texts = []\n",
    "            for artifact in artifacts:\n",
    "                for p in artifact.get(\"parts\", []):\n",
    "                    if p.get(\"kind\") == \"text\":\n",
    "                        collected_texts.append(p.get(\"text\"))\n",
    "            steps.append(f\"Agent response: {' '.join(collected_texts)}\")\n",
    "\n",
    "output_parts = y[\"output\"][\"content\"].get(\"parts\", [])\n",
    "collected_texts = []\n",
    "for part in output_parts:\n",
    "    if \"function_call\" in part:\n",
    "        args = part[\"function_call\"][\"args\"]\n",
    "        collected_texts.append(f\"Agent '{args.get('agent_name')}' was assigned a task {args.get('task')}\")\n",
    "\n",
    "steps.append(f\"Final {' '.join(collected_texts)}\")\n",
    "\n",
    "pprint.pprint(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "885f6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_context = lines+steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2db5f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a=pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73bab3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>3c7300a2c603e23e4cd87f07cdc5c34f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>timestamp</td>\n",
       "      <td>2025-11-20 09:28:20.473000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>name</td>\n",
       "      <td>call_llm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>input</td>\n",
       "      <td>{'model': 'azure/gpt-4o-mini', 'config': {'sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>output</td>\n",
       "      <td>{'content': {'parts': [{'function_call': {'id'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>session_id</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>release</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>version</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>user_id</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>metadata</td>\n",
       "      <td>{'attributes': {'gen_ai.system': 'gcp.vertex.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tags</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>public</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>environment</td>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>html_path</td>\n",
       "      <td>/project/cmhwzwyvs01fjad07xfmlp94h/traces/3c73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>latency</td>\n",
       "      <td>8.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>total_cost</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>observations</td>\n",
       "      <td>[id='6eda86706e8e2b96' trace_id='3c7300a2c603e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>scores</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>externalId</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bookmarked</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>createdAt</td>\n",
       "      <td>2025-11-20T09:28:27.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>updatedAt</td>\n",
       "      <td>2025-11-20T09:28:32.069Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>projectId</td>\n",
       "      <td>cmhwzwyvs01fjad07xfmlp94h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0                                                  1\n",
       "0             id                   3c7300a2c603e23e4cd87f07cdc5c34f\n",
       "1      timestamp                   2025-11-20 09:28:20.473000+00:00\n",
       "2           name                                           call_llm\n",
       "3          input  {'model': 'azure/gpt-4o-mini', 'config': {'sys...\n",
       "4         output  {'content': {'parts': [{'function_call': {'id'...\n",
       "5     session_id                                               None\n",
       "6        release                                               None\n",
       "7        version                                               None\n",
       "8        user_id                                               None\n",
       "9       metadata  {'attributes': {'gen_ai.system': 'gcp.vertex.a...\n",
       "10          tags                                                 []\n",
       "11        public                                              False\n",
       "12   environment                                            default\n",
       "13     html_path  /project/cmhwzwyvs01fjad07xfmlp94h/traces/3c73...\n",
       "14       latency                                              8.731\n",
       "15    total_cost                                                0.0\n",
       "16  observations  [id='6eda86706e8e2b96' trace_id='3c7300a2c603e...\n",
       "17        scores                                                 []\n",
       "18    externalId                                               None\n",
       "19    bookmarked                                              False\n",
       "20     createdAt                           2025-11-20T09:28:27.000Z\n",
       "21     updatedAt                           2025-11-20T09:28:32.069Z\n",
       "22     projectId                          cmhwzwyvs01fjad07xfmlp94h"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f91b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deepeval.tracing import observe, update_current_trace\n",
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "from deepeval.metrics import PlanAdherenceMetric\n",
    "from deepeval.test_case import ToolCall\n",
    "\n",
    "\n",
    "@observe\n",
    "def tool_call(input):\n",
    "    ...\n",
    "    return [ToolCall(name=\"CheckWhether\")]\n",
    "\n",
    "@observe\n",
    "def agent(input):\n",
    "    tools = tool_call(input)\n",
    "    output = llm(input, tools)\n",
    "    update_current_trace(\n",
    "        input=input,\n",
    "        output=output,\n",
    "        tools_called=tools\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = EvaluationDataset(goldens=[Golden(input=\"What's the weather like in SF?\")])\n",
    "\n",
    "# Initialize metric\n",
    "metric = PlanAdherenceMetric(threshold=0.7, model=\"gpt-4o\",tools)\n",
    "\n",
    "# Loop through dataset\n",
    "for golden in dataset.evals_iterator(metrics=[metric]):\n",
    "    agent(golden.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DeepEval evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Multi-Agent Orchestration Quality </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mMulti-Agent Orchestration Quality \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\n",
       "\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Efficiency &amp; Minimal Steps </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mEfficiency & Minimal Steps \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Plan Adherence Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mPlan Adherence Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\rich\\live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\rich\\live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MissingTestCaseParamsError",
     "evalue": "'tools_called' cannot be None for the 'Plan Adherence' metric",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingTestCaseParamsError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 74\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# ===============================================================\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 6. RUN\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# ===============================================================\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 74\u001b[0m     \u001b[43mevaluate_and_log\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 66\u001b[0m, in \u001b[0;36mevaluate_and_log\u001b[1;34m(input, retrieval_context, context, output)\u001b[0m\n\u001b[0;32m     63\u001b[0m metrics \u001b[38;5;241m=\u001b[39m get_metrics()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning DeepEval evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ← Correct call\u001b[39;00m\n\u001b[0;32m     67\u001b[0m result \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mtest_results[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     68\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(result)\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\deepeval\\evaluate\\evaluate.py:236\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(test_cases, metrics, metric_collection, hyperparameters, identifier, async_config, display_config, cache_config, error_config)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m async_config\u001b[38;5;241m.\u001b[39mrun_async:\n\u001b[0;32m    235\u001b[0m     loop \u001b[38;5;241m=\u001b[39m get_or_create_event_loop()\n\u001b[1;32m--> 236\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43ma_execute_test_cases\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m            \u001b[49m\u001b[43midentifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m            \u001b[49m\u001b[43merror_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisplay_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m            \u001b[49m\u001b[43masync_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m execute_test_cases(\n\u001b[0;32m    249\u001b[0m         test_cases,\n\u001b[0;32m    250\u001b[0m         metrics,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m         cache_config\u001b[38;5;241m=\u001b[39mcache_config,\n\u001b[0;32m    255\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\deepeval\\evaluate\\execute.py:708\u001b[0m, in \u001b[0;36ma_execute_test_cases\u001b[1;34m(test_cases, metrics, error_config, display_config, cache_config, async_config, identifier, test_run_manager, _use_bar_indicator, _is_assert_test)\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(async_config\u001b[38;5;241m.\u001b[39mthrottle_value)\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(\n\u001b[0;32m    709\u001b[0m         asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks),\n\u001b[0;32m    710\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m_gather_timeout(),\n\u001b[0;32m    711\u001b[0m     )\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (asyncio\u001b[38;5;241m.\u001b[39mTimeoutError, \u001b[38;5;167;01mTimeoutError\u001b[39;00m):\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tasks:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py:447\u001b[0m, in \u001b[0;36mwait_for\u001b[1;34m(fut, timeout)\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    449\u001b[0m     fut\u001b[38;5;241m.\u001b[39mremove_done_callback(cb)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\deepeval\\evaluate\\execute.py:585\u001b[0m, in \u001b[0;36ma_execute_test_cases.<locals>.execute_with_semaphore\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[0;32m    584\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m _per_task_timeout()\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _await_with_outer_deadline(\n\u001b[0;32m    586\u001b[0m         func, \u001b[38;5;241m*\u001b[39margs, timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    587\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\deepeval\\evaluate\\execute.py:255\u001b[0m, in \u001b[0;36m_await_with_outer_deadline\u001b[1;34m(obj, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m         coro \u001b[38;5;241m=\u001b[39m obj(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     reset_outer_deadline(token)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py:447\u001b[0m, in \u001b[0;36mwait_for\u001b[1;34m(fut, timeout)\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    449\u001b[0m     fut\u001b[38;5;241m.\u001b[39mremove_done_callback(cb)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py:234\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    232\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\deepeval\\evaluate\\execute.py:855\u001b[0m, in \u001b[0;36m_a_execute_llm_test_cases\u001b[1;34m(metrics, test_case, test_run_manager, test_results, count, test_run, ignore_errors, skip_on_missing_params, use_cache, show_indicator, _use_bar_indicator, _is_assert_test, progress, pbar_id)\u001b[0m\n\u001b[0;32m    852\u001b[0m     new_cached_test_case: CachedTestCase \u001b[38;5;241m=\u001b[39m CachedTestCase()\n\u001b[0;32m    853\u001b[0m     test_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m--> 855\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m measure_metrics_with_indicator(\n\u001b[0;32m    856\u001b[0m         metrics\u001b[38;5;241m=\u001b[39mmetrics,\n\u001b[0;32m    857\u001b[0m         test_case\u001b[38;5;241m=\u001b[39mtest_case,\n\u001b[0;32m    858\u001b[0m         cached_test_case\u001b[38;5;241m=\u001b[39mcached_test_case,\n\u001b[0;32m    859\u001b[0m         skip_on_missing_params\u001b[38;5;241m=\u001b[39mskip_on_missing_params,\n\u001b[0;32m    860\u001b[0m         ignore_errors\u001b[38;5;241m=\u001b[39mignore_errors,\n\u001b[0;32m    861\u001b[0m         show_indicator\u001b[38;5;241m=\u001b[39mshow_metrics_indicator,\n\u001b[0;32m    862\u001b[0m         pbar_eval_id\u001b[38;5;241m=\u001b[39mpbar_test_case_id,\n\u001b[0;32m    863\u001b[0m         progress\u001b[38;5;241m=\u001b[39mprogress,\n\u001b[0;32m    864\u001b[0m     )\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[0;32m    866\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimed out/cancelled while evaluating metric. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    868\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncrease DEEPEVAL_PER_TASK_TIMEOUT_SECONDS_OVERRIDE or set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEEPEVAL_LOG_STACK_TRACES=1 for full traceback.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    870\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\deepeval\\metrics\\indicator.py:237\u001b[0m, in \u001b[0;36mmeasure_metrics_with_indicator\u001b[1;34m(metrics, test_case, cached_test_case, ignore_errors, skip_on_missing_params, show_indicator, progress, pbar_eval_id, _in_component)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m         tasks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    226\u001b[0m             safe_a_measure(\n\u001b[0;32m    227\u001b[0m                 metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m             )\n\u001b[0;32m    235\u001b[0m         )\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\deepeval\\metrics\\indicator.py:250\u001b[0m, in \u001b[0;36msafe_a_measure\u001b[1;34m(metric, tc, ignore_errors, skip_on_missing_params, progress, pbar_eval_id, _in_component)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msafe_a_measure\u001b[39m(\n\u001b[0;32m    241\u001b[0m     metric: Union[BaseMetric, BaseMultimodalMetric, BaseConversationalMetric],\n\u001b[0;32m    242\u001b[0m     tc: Union[LLMTestCase, MLLMTestCase, ConversationalTestCase],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     _in_component: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    248\u001b[0m ):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m metric\u001b[38;5;241m.\u001b[39ma_measure(\n\u001b[0;32m    251\u001b[0m             tc,\n\u001b[0;32m    252\u001b[0m             _show_indicator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    253\u001b[0m             _in_component\u001b[38;5;241m=\u001b[39m_in_component,\n\u001b[0;32m    254\u001b[0m             _log_metric_to_confident\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    255\u001b[0m         )\n\u001b[0;32m    256\u001b[0m         update_pbar(progress, pbar_eval_id)\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\deepeval\\metrics\\plan_adherence\\plan_adherence.py:123\u001b[0m, in \u001b[0;36mPlanAdherenceMetric.a_measure\u001b[1;34m(self, test_case, _show_indicator, _in_component, _log_metric_to_confident)\u001b[0m\n\u001b[0;32m    121\u001b[0m has_trace: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(test_case\u001b[38;5;241m.\u001b[39m_trace_dict, Dict)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_trace:\n\u001b[1;32m--> 123\u001b[0m     \u001b[43mcheck_llm_test_case_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_required_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_native_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m metric_progress_indicator(\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    129\u001b[0m     async_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    130\u001b[0m     _show_indicator\u001b[38;5;241m=\u001b[39m_show_indicator,\n\u001b[0;32m    131\u001b[0m     _in_component\u001b[38;5;241m=\u001b[39m_in_component,\n\u001b[0;32m    132\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\deepeval\\metrics\\utils.py:260\u001b[0m, in \u001b[0;36mcheck_llm_test_case_params\u001b[1;34m(test_case, test_case_params, metric)\u001b[0m\n\u001b[0;32m    258\u001b[0m error_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_params_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be None for the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m metric\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    259\u001b[0m metric\u001b[38;5;241m.\u001b[39merror \u001b[38;5;241m=\u001b[39m error_str\n\u001b[1;32m--> 260\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MissingTestCaseParamsError(error_str)\n",
      "\u001b[1;31mMissingTestCaseParamsError\u001b[0m: 'tools_called' cannot be None for the 'Plan Adherence' metric"
     ]
    }
   ],
   "source": [
    "# FINAL_WORKING_DEEPEVAL_NOV19_2025.py\n",
    "import ast\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# DeepEval - latest 2025 API\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import GEval, FaithfulnessMetric, ToxicityMetric,PlanAdherenceMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.test_case import LLMTestCaseParams  # ← CRITICAL\n",
    "import pprint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 3. Build test case\n",
    "# ===============================================================\n",
    "def build_test_case(input,retrieval_context,context,output) -> LLMTestCase:\n",
    "\n",
    "    return LLMTestCase(\n",
    "        input=input,\n",
    "        actual_output=output,\n",
    "        retrieval_context=retrieval_context,\n",
    "        context=context,\n",
    "    )\n",
    "\n",
    "# ===============================================================\n",
    "# 4. METRICS — ONLY THIS WAY WORKS IN NOV 2025\n",
    "# ===============================================================\n",
    "def get_metrics():\n",
    "    return [\n",
    "        GEval(\n",
    "            name=\"Multi-Agent Orchestration Quality\",\n",
    "            criteria=\"Rate how well the routing agent orchestrated other agents using the trace in retrieval_context.\",\n",
    "            evaluation_params=[\n",
    "                LLMTestCaseParams.INPUT,\n",
    "                LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "                LLMTestCaseParams.RETRIEVAL_CONTEXT,\n",
    "            ],\n",
    "            model=\"gpt-4o-mini\",\n",
    "            threshold=0.8\n",
    "        ),\n",
    "        GEval(\n",
    "            name=\"Efficiency & Minimal Steps\",\n",
    "            criteria=\"Was the task solved in the absolute minimum number of steps?\",\n",
    "            evaluation_params=[\n",
    "                LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "                LLMTestCaseParams.RETRIEVAL_CONTEXT,\n",
    "            ],\n",
    "            model=\"gpt-4o-mini\",\n",
    "            threshold=0.9\n",
    "        ),\n",
    "        FaithfulnessMetric(threshold=0.9,model=\"gpt-4o-mini\"),\n",
    "        # ToxicityMetric(threshold=0.1,model =\"gpt-4o-mini\"),\n",
    "        PlanAdherenceMetric(threshold=0.5,model=\"gpt-4o-mini\"),\n",
    "    ]\n",
    "\n",
    "# ===============================================================\n",
    "# 5. Evaluate + Log\n",
    "# ===============================================================\n",
    "def evaluate_and_log(input,retrieval_context,context,output):\n",
    "    test_case = build_test_case(input,retrieval_context,context,output)\n",
    "    metrics = get_metrics()\n",
    "\n",
    "    print(\"Running DeepEval evaluation...\")\n",
    "    results = evaluate(test_cases=[test_case], metrics=metrics)  # ← Correct call\n",
    "    result = results.test_results[0]\n",
    "    pprint.pprint(result)\n",
    "\n",
    "# ===============================================================\n",
    "# 6. RUN\n",
    "# ===============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_and_log(input,steps,lines,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bde34",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You set x = ..., replace it with the actual trace dictionary!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(x)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# You probably typed x = ... by mistake\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou set x = ..., replace it with the actual trace dictionary!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Now x is guaranteed to be the correct dict\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: You set x = ..., replace it with the actual trace dictionary!"
     ]
    }
   ],
   "source": [
    "# === ONE-TIME FIX: Turn whatever x is into the real dictionary ===\n",
    "import json\n",
    "import ast\n",
    "\n",
    "if isinstance(x, str):\n",
    "    # If you pasted the JSON as a string\n",
    "    try:\n",
    "        x = json.loads(x)\n",
    "    except:\n",
    "        x = ast.literal_eval(x)\n",
    "elif isinstance(x, set):\n",
    "    x = list(x)[0]\n",
    "elif x is Ellipsis:\n",
    "    # You probably typed x = ... by mistake\n",
    "    raise ValueError(\"You set x = ..., replace it with the actual trace dictionary!\")\n",
    "# Now x is guaranteed to be the correct dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2dffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577268f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 1. Extract the user query (always the first plain text from user)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      6\u001b[0m user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m      7\u001b[0m     (p[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m contents \n\u001b[0;32m      8\u001b[0m      \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown query\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'ellipsis' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d53d0e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute '_dataset_rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Golden, EvaluationDataset\n\u001b[1;32m----> 2\u001b[0m final_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluationDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoldens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m metric \u001b[38;5;241m=\u001b[39m PlanAdherenceMetric(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Loop through dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\deepeval\\dataset\\dataset.py:98\u001b[0m, in \u001b[0;36mEvaluationDataset.__init__\u001b[1;34m(self, goldens)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conversational_goldens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m golden \u001b[38;5;129;01min\u001b[39;00m goldens:\n\u001b[1;32m---> 98\u001b[0m     \u001b[43mgolden\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_rank\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(goldens)\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_turn:\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_conversational_golden(golden)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute '_dataset_rank'"
     ]
    }
   ],
   "source": [
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "final_dataset = EvaluationDataset(goldens=a)\n",
    "metric = PlanAdherenceMetric(threshold=0.7, model=\"gpt-4o-mini\")\n",
    "# Loop through dataset\n",
    "for golden in final_dataset.evals_iterator(metrics=[metric]):\n",
    "    agent(golden.input)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bed73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL_WORKING_DEEPEVAL_NOV19_2025.py\n",
    "import ast\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# DeepEval - latest 2025 API\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import GEval, FaithfulnessMetric, ToxicityMetric,PlanAdherenceMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.test_case import LLMTestCaseParams  # ← CRITICAL\n",
    "import pprint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 3. Build test case\n",
    "# ===============================================================\n",
    "def build_test_case(input,retrieval_context,context,output) -> LLMTestCase:\n",
    "\n",
    "    return TestCase(\n",
    "        input=input,\n",
    "        actual_output=output,\n",
    "        retrieval_context=retrieval_context,\n",
    "        context=context,\n",
    "    )\n",
    "\n",
    "# ===============================================================\n",
    "# 4. METRICS — ONLY THIS WAY WORKS IN NOV 2025\n",
    "# ===============================================================\n",
    "def get_metrics():\n",
    "    return [\n",
    "        GEval(\n",
    "            name=\"Multi-Agent Orchestration Quality\",\n",
    "            criteria=\"Rate how well the routing agent orchestrated other agents using the trace in retrieval_context.\",\n",
    "            evaluation_params=[\n",
    "                LLMTestCaseParams.INPUT,\n",
    "                LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "                LLMTestCaseParams.RETRIEVAL_CONTEXT,\n",
    "            ],\n",
    "            model=\"gpt-4o-mini\",\n",
    "            threshold=0.8\n",
    "        ),\n",
    "        GEval(\n",
    "            name=\"Efficiency & Minimal Steps\",\n",
    "            criteria=\"Was the task solved in the absolute minimum number of steps?\",\n",
    "            evaluation_params=[\n",
    "                LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "                LLMTestCaseParams.RETRIEVAL_CONTEXT,\n",
    "            ],\n",
    "            model=\"gpt-4o-mini\",\n",
    "            threshold=0.9\n",
    "        ),\n",
    "        FaithfulnessMetric(threshold=0.9,model=\"gpt-4o-mini\"),\n",
    "        # ToxicityMetric(threshold=0.1,model =\"gpt-4o-mini\"),\n",
    "        PlanAdherenceMetric(threshold=0.5,model=\"gpt-4o-mini\"),\n",
    "    ]\n",
    "\n",
    "# ===============================================================\n",
    "# 5. Evaluate + Log\n",
    "# ===============================================================\n",
    "def evaluate_and_log(input,retrieval_context,context,output):\n",
    "    test_case = build_test_case(input,retrieval_context,context,output)\n",
    "    metrics = get_metrics()\n",
    "\n",
    "    print(\"Running DeepEval evaluation...\")\n",
    "    results = evaluate(test_cases=[test_case], metrics=metrics)  # ← Correct call\n",
    "    result = results.test_results[0]\n",
    "    pprint.pprint(result)\n",
    "\n",
    "# ===============================================================\n",
    "# 6. RUN\n",
    "# ===============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_and_log(input,steps,lines,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f33e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import TaskCompletionMetric\n",
    "from deepeval.test_case import LLMTestCase  # For non-dataset eval\n",
    "\n",
    "# Initialize the metric\n",
    "task_metric = TaskCompletionMetric(\n",
    "    threshold=0.7,  # Pass if score >= 0.7 (customize as needed)\n",
    "    model=\"gpt-4o\",  # Or your preferred LLM for judging\n",
    "    include_reason=True,  # Outputs explanation for the score\n",
    "    strict_mode=False,  # Use False for nuanced scoring (0-1); True for binary pass/fail\n",
    "    async_mode=True,  # Faster for batches\n",
    "    verbose_mode=True  # Prints details during eval (optional)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0238aa99",
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingTestCaseParamsError",
     "evalue": "'tools_called' cannot be None for the 'Task Completion' metric",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingTestCaseParamsError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 17\u001b[0m\n\u001b[0;32m      9\u001b[0m test_case \u001b[38;5;241m=\u001b[39m LLMTestCase(\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(x\u001b[38;5;241m.\u001b[39minput),  \u001b[38;5;66;03m# Task/query from trace\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     actual_output\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(x\u001b[38;5;241m.\u001b[39moutput),  \u001b[38;5;66;03m# Agent's final outcome\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# retrieval_context=trace.spans if needed for RAG agents (optional)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# expected_output=None  # Not needed for referenceless metrics like this\u001b[39;00m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mtask_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask Completion Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 0-1 float\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReason: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_metric\u001b[38;5;241m.\u001b[39mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# Explanation from the LLM judge\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\deepeval\\metrics\\task_completion\\task_completion.py:63\u001b[0m, in \u001b[0;36mTaskCompletionMetric.measure\u001b[1;34m(self, test_case, _show_indicator, _in_component, _log_metric_to_confident)\u001b[0m\n\u001b[0;32m     61\u001b[0m has_trace: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(test_case\u001b[38;5;241m.\u001b[39m_trace_dict, Dict)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_trace:\n\u001b[1;32m---> 63\u001b[0m     \u001b[43mcheck_llm_test_case_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_required_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_native_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m metric_progress_indicator(\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m, _show_indicator\u001b[38;5;241m=\u001b[39m_show_indicator, _in_component\u001b[38;5;241m=\u001b[39m_in_component\n\u001b[0;32m     68\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\RohitKaushal\\OneDrive - Xebia\\Desktop\\Code_execution_mcp\\venv\\lib\\site-packages\\deepeval\\metrics\\utils.py:260\u001b[0m, in \u001b[0;36mcheck_llm_test_case_params\u001b[1;34m(test_case, test_case_params, metric)\u001b[0m\n\u001b[0;32m    258\u001b[0m error_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_params_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be None for the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m metric\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    259\u001b[0m metric\u001b[38;5;241m.\u001b[39merror \u001b[38;5;241m=\u001b[39m error_str\n\u001b[1;32m--> 260\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MissingTestCaseParamsError(error_str)\n",
      "\u001b[1;31mMissingTestCaseParamsError\u001b[0m: 'tools_called' cannot be None for the 'Task Completion' metric"
     ]
    }
   ],
   "source": [
    "# from langfuse import Langfuse\n",
    "\n",
    "# langfuse_client = Langfuse()\n",
    "\n",
    "# # Fetch the latest trace (or by ID: langfuse_client.get_trace(trace_id))\n",
    "# trace = langfuse_client.get(limit=1)[0]  # Assumes your latest run; filter as needed\n",
    "\n",
    "# Create a test case from the Langfuse trace\n",
    "test_case = LLMTestCase(\n",
    "    input=json.dumps(x.input),  # Task/query from trace\n",
    "    actual_output=json.dumps(x.output),  # Agent's final outcome\n",
    "    # retrieval_context=trace.spans if needed for RAG agents (optional)\n",
    "    # expected_output=None  # Not needed for referenceless metrics like this\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "score = task_metric.measure(test_case)\n",
    "print(f\"Task Completion Score: {score}\")  # 0-1 float\n",
    "print(f\"Reason: {task_metric.reason}\")   # Explanation from the LLM judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a674be07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
